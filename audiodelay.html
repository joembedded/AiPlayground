<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
<h3>Audio Delay Test</h3>
<p>This test captures audio from the microphone, applies a delay effect, and plays it back with 
    a delay (here 3 seconds), but analysis is the realtime signal.
</p>
<div>
    <canvas class="visualizer" width="400" height="50" style="border: 1px solid gray;"></canvas>
</div>
<div>
    <audio id="myPlayer" controls autoplay></audio><br>
    <button onclick="startDelay()">Start Delay</button>
</div>


</body>
<script>

let dataArray = null;
let analyser = null;

// RMS-Berechnung (aus dataArray, byteArray: 0..255, 128 ist "0")
function computeRMSFromTimeDomain(byteArray) {
    let sumSq = 0;
    const length = byteArray.length;
    for (let i = 0; i < length; i++) {
        const v = (byteArray[i] - 128) / 128; // -1..1
        sumSq += v * v;
    }
    return Math.sqrt(sumSq / length);
}

// animation
const canvas = document.querySelector(".visualizer");
const canvasCtx = canvas.getContext("2d");

function frameMonitor() {
    frameMoniId = requestAnimationFrame(frameMonitor);

    analyser.getByteTimeDomainData(dataArray);
    const frameRms = computeRMSFromTimeDomain(dataArray);

    // Zeichnen
    const w = canvas.width, h = canvas.height;
    // Hintergrund
    canvasCtx.fillStyle = '#fafafa';
    canvasCtx.fillRect(0, 0, w, h);

    canvasCtx.fillStyle = 'lime';
    canvasCtx.fillRect(10, 5, frameRms * w * 3, 10);

    // Text
    canvasCtx.fillStyle = 'black';
    canvasCtx.font = '14px system-ui';
    canvasCtx.fillText(`RMS:${frameRms.toFixed(3)}`, 10, 40);
}


async function startDelay() {
        try {
            // 1. AudioContext
            const audioContext = new AudioContext();
            const delaySeconds = 3;
            // 2. MediaStream (z. B. Mikrofon)
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            // 3. Source Node
            const source = audioContext.createMediaStreamSource(stream);
            // 4. DelayNode (maxDelayTime in Sekunden)
            const delayNode = audioContext.createDelay(delaySeconds); // Def. ist 1
            delayNode.delayTime.value = delaySeconds; // sec
            // 5. Ziel-Stream
            const destination = audioContext.createMediaStreamDestination();
            // 6. Verbinden
            source.connect(delayNode);
            delayNode.connect(destination); // Das Ziel ist der verzÃ¶gerte Stream
            // ðŸ‘‰ Der verzÃ¶gerte Stream:
            const delayedStream = destination.stream;

            const audio = document.getElementById("myPlayer");
            audio.srcObject = delayedStream;
            audio.autoplay = true;

            // Optional: Visualizer (RealTime)
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 2048; // keine FFT, hier nur Zeitbereich
            dataArray = new Uint8Array(analyser.fftSize);
            source.connect(analyser);

            frameMonitor();
        } catch (e) {
            console.error('ERROR(startDelay):', e);
        }
    }

</script>

</html>