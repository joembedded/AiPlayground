<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>

<body>
    <h3>Audio Delay Test</h3>
    <p>This test captures audio from the microphone, applies a delay effect, and plays it back with
        a delay (here 3 seconds), but analysis is possible with the realtime signal.
    </p>
    <div>
        <canvas class="visualizer" width="400" height="50" style="border: 1px solid gray;"></canvas>
    </div>
    <div>
        <audio id="myPlayer" controls autoplay></audio>
        <audio id="myRecorder" controls ></audio>
        <br>
        <button onclick="startDelay()">Start Delay</button>
        <button onclick="startRecord()">Start Record</button>
    </div>
    <div id="preDiv" style="background-color: aliceblue;">---Info:---<br></div>
</body>
<script>

const infoDiv = document.getElementById("preDiv");

// Array für Analyser:
    let dataArray = null;
    // Streams:
    let analyser = null;
    let streamSource = null;
    let delayNode = null;
    let delayedStream = null;
    const useMimeStream =  'audio/webm; codecs=opus';
    const useMimeBlob =  'audio/ogg; codecs=opus';


    // RMS-Berechnung (aus dataArray, byteArray: 0..255, 128 ist "0")
    function computeRMSFromTimeDomain(byteArray) {
        let sumSq = 0;
        const length = byteArray.length;
        for (let i = 0; i < length; i++) {
            const v = (byteArray[i] - 128) / 128; // -1..1
            sumSq += v * v;
        }
        return Math.sqrt(sumSq / length);
    }

    // animation
    const canvas = document.querySelector(".visualizer");
    const canvasCtx = canvas.getContext("2d");

    function frameMonitor() {
        frameMoniId = requestAnimationFrame(frameMonitor);

        analyser.getByteTimeDomainData(dataArray);
        const frameRms = computeRMSFromTimeDomain(dataArray);

        // Zeichnen
        const w = canvas.width, h = canvas.height;
        // Hintergrund
        canvasCtx.fillStyle = 'whitesmoke';
        canvasCtx.fillRect(0, 0, w, h);

        canvasCtx.fillStyle = 'lime';
        canvasCtx.fillRect(10, 5, frameRms * w * 3, 10);

        // Text
        canvasCtx.fillStyle = 'black';
        canvasCtx.font = '14px system-ui';
        canvasCtx.fillText(`RMS:${frameRms.toFixed(3)}`, 10, 40);
    }

    async function startRecord(params) {
        try {
            if (!MediaRecorder.isTypeSupported(useMimeStream)) throw new Error(`MIME-Type not supported: ${useMimeStream}`);
            const recorder = new MediaRecorder(delayedStream, { mimeType: useMimeStream });
            // Zeichen den delayedStream auf
            infoDiv.innerHTML += "Zeichne den delayedStream auf<br>";
            const chunks = [];
            recorder.ondataavailable = (e) => chunks.push(e.data);
            recorder.onstop = () => {
                const blob = new Blob(chunks, { type: useMimeBlob });
                const url = URL.createObjectURL(blob);
                infoDiv.innerHTML += "Aufnahme bereit: " + url + "<br>";
                const myRecorder = document.getElementById("myRecorder");
                myRecorder.src = url;
            };

            recorder.start();
            infoDiv.innerHTML += "Aufnahme läuft...<br>";

            setTimeout(() => {
                recorder.stop()
                infoDiv.innerHTML += "Aufnahme gestoppt.<br>";
            }, 6000);
        } catch (e) {
            infoDiv.innerHTML += 'ERROR(startRecord): ' + e.message + '<br>';
        }
    }

    async function startDelay() {
        try {
            // 1. AudioContext
            const audioContext = new AudioContext();
            const delaySeconds = 3;
            // 2. MediaStream (z. B. Mikrofon)
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            // 3. Source Node
            streamSource = audioContext.createMediaStreamSource(stream);
            // 4. DelayNode (maxDelayTime in Sekunden)
            delayNode = audioContext.createDelay(delaySeconds); // Def. ist 1
            delayNode.delayTime.value = delaySeconds; // sec
            // 5. Ziel-Stream
            const destination = audioContext.createMediaStreamDestination();
            // 6. Verbinden
            streamSource.connect(delayNode);
            delayNode.connect(destination); // Das Ziel ist der verzögerte Stream
            // Der verzögerte Stream:
            delayedStream = destination.stream;

            const audio = document.getElementById("myPlayer");
            audio.srcObject = delayedStream;
            audio.autoplay = true;
            // Optional: Visualizer (RealTime)
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 2048; // keine FFT, hier nur Zeitbereich
            dataArray = new Uint8Array(analyser.fftSize);
            streamSource.connect(analyser);

            frameMonitor();
            infoDiv.innerHTML += 'Audio Delay started. Speak into the microphone.<br>';
        } catch (e) {
            infoDiv.innerHTML += 'ERROR(startDelay): ' + e.message + '<br>';
        }
    }

</script>

</html>